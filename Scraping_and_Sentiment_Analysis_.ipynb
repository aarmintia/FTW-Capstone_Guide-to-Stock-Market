{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarmintia/FTW-Capstone_Guide-to-Stock-Market/blob/main/Scraping_and_Sentiment_Analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701ec450",
      "metadata": {
        "id": "701ec450"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b38fc3b",
      "metadata": {
        "id": "4b38fc3b"
      },
      "source": [
        "## Code for Scraping Inquirer for headlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745ca764",
      "metadata": {
        "id": "745ca764"
      },
      "outputs": [],
      "source": [
        "# Function to scrape headlines for a given date\n",
        "def scrape_headlines_for_date(date):\n",
        "    url = f'https://www.inquirer.net/article-index/?d={date.year}-{date.strftime(\"%m\")}-{date.strftime(\"%d\")}'\n",
        "\n",
        "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'})\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        i = soup.find('h4', string=\"BUSINESS\")\n",
        "        k = i.text\n",
        "        ul = i.findNext('ul')\n",
        "        v = [li.text for li in ul.findAll('li')]\n",
        "        output = v\n",
        "\n",
        "        return output\n",
        "    else:\n",
        "        print(f\"Failed to fetch data for {date}. Status code: {response.status_code}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1a27ca",
      "metadata": {
        "id": "cc1a27ca"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970d7de3",
      "metadata": {
        "id": "970d7de3"
      },
      "outputs": [],
      "source": [
        "def cleanInquirerData(data): # Function to remove last 3 words in the string [i.e. 5 years ago]\n",
        "    clean_data = []\n",
        "    for i in data:\n",
        "        temp = i.split()\n",
        "        temp = temp[:-3]\n",
        "        new_string = ' '.join(temp)\n",
        "        clean_data.append(new_string)\n",
        "    return clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e16db5",
      "metadata": {
        "id": "b2e16db5"
      },
      "outputs": [],
      "source": [
        "def filterDataBasedOnKeywords(data, keywords):\n",
        "    data = [x.lower() for x in data]\n",
        "    keywords = [x.lower() for x in keywords]\n",
        "\n",
        "    clean_data = []\n",
        "\n",
        "    for i in data:\n",
        "        # Use regex to find whole words and check if any keyword is a standalone word in the string\n",
        "        truthy = any(re.search(rf'\\b{re.escape(ele)}\\b', i) for ele in keywords)\n",
        "\n",
        "        if truthy:\n",
        "            clean_data.append(i)\n",
        "\n",
        "    return clean_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4426d5a7",
      "metadata": {
        "id": "4426d5a7"
      },
      "source": [
        "### Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b58746",
      "metadata": {
        "id": "88b58746"
      },
      "outputs": [],
      "source": [
        "def getHeadlinesAsDictionary(start_date, end_date, keywords):\n",
        "    data = {}\n",
        "    current_date = start_date\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        print(current_date)\n",
        "        key = f'{current_date.year}-{current_date.strftime(\"%m\")}-{current_date.strftime(\"%d\")}'\n",
        "\n",
        "        # Call webscraping function\n",
        "        headlines = scrape_headlines_for_date(current_date)\n",
        "\n",
        "        # Call data processing functions\n",
        "        headlines = cleanInquirerData(headlines)\n",
        "        headlines = filterDataBasedOnKeywords(headlines, keywords)\n",
        "\n",
        "        # Transform data\n",
        "        data_dict = {}\n",
        "        data_dict['headlines'] = headlines\n",
        "        data_dict['sentiments'] = []\n",
        "        data[key] = data_dict\n",
        "\n",
        "        # Go next day\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc45c52",
      "metadata": {
        "id": "bbc45c52"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def convertToJson(data, filename):\n",
        "    with open(filename, \"w\") as outfile:\n",
        "        json.dump(data, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec52e7be",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec52e7be",
        "outputId": "6c1fe9f0-3e6e-406e-e359-63e615b5ccf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n",
            "2023-11-24 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# Define Variables\n",
        "start_date = datetime(2018, 1, 1)\n",
        "end_date = datetime(2023, 11, 24)\n",
        "keywords_dict = {\n",
        "    'AC': ['ayala corp', 'ac'],\n",
        "    'ACEN': ['acen', 'acen corporation', 'ac energy'],\n",
        "    'AEV': ['AEV', 'aboitiz'],\n",
        "    'AGI': ['agi', 'alliance global'],\n",
        "    'ALI': ['ali', 'ayala land'],\n",
        "    'BDO': ['bdo', 'unibank'],\n",
        "    'BLOOM': ['bloomberry', 'bloom resort'],\n",
        "    'BPI': ['bpi', 'bank of the philippine islands'],\n",
        "    'CNPF': ['cnpf', 'century pacific'],\n",
        "    'CNVRG': ['cnvrg', 'converge'],\n",
        "    'DMC': ['dmc', 'dmci', 'consunji'],\n",
        "    'EMI': ['emi', 'emperador'],\n",
        "    'GLO': ['glo', 'globe'],\n",
        "    'GTCAP': ['gtcap', 'gt', 'gt capital'],\n",
        "    'ICT': ['ict', 'international container terminal', 'container terminal service'],\n",
        "    'JFC': ['jfc', 'jollibee'],\n",
        "    'JGS': ['jgs', 'jg summit'],\n",
        "    'LTG': ['ltg', 'lt group'],\n",
        "    'MBT': ['mbt', 'metrobank', 'metropolitan bank'],\n",
        "    'MER': ['mer', 'manila electric', 'meralco'],\n",
        "    'MONDE': ['monde', 'nissin'],\n",
        "    'NIKL': ['nikl', 'nickel asia', 'nickel'],\n",
        "    'PGOLD': ['pgold', 'puregold'],\n",
        "    'SCC': ['scc', 'semirara'],\n",
        "    'SM': ['sm', 'sm investments'],\n",
        "    'SMC': ['smc', 'san miguel'],\n",
        "    'SMPH': ['smph', 'sm prime'],\n",
        "    'TEL': ['PLDT', 'tel'],\n",
        "    'URC': ['urc', 'universal robina'],\n",
        "    'WLCON': ['wlcon', 'wilcon'],\n",
        "}\n",
        "# Scrape headlines for the date range\n",
        "data_ac = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['AC'])\n",
        "data_acen = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['ACEN'])\n",
        "data_aev = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['AEV'])\n",
        "data_agi = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['AGI'])\n",
        "data_ali = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['ALI'])\n",
        "data_bdo = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['BDO'])\n",
        "data_bloom = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['BLOOM'])\n",
        "data_bpi = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['BPI'])\n",
        "data_cnpf = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['CNPF'])\n",
        "data_cnvrg = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['CNVRG'])\n",
        "data_dmc = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['DMC'])\n",
        "data_emi = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['EMI'])\n",
        "data_glo = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['GLO'])\n",
        "data_gtcap = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['GTCAP'])\n",
        "data_ict = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['ICT'])\n",
        "data_jfc = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['JFC'])\n",
        "data_jgs = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['JGS'])\n",
        "data_ltg = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['LTG'])\n",
        "data_mbt = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['MBT'])\n",
        "data_mer = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['MER'])\n",
        "data_monde = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['MONDE'])\n",
        "data_nikl = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['NIKL'])\n",
        "data_pgold = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['PGOLD'])\n",
        "data_scc = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['SCC'])\n",
        "data_sm = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['SM'])\n",
        "data_smc = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['SMC'])\n",
        "data_smph = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['SMPH'])\n",
        "data_tel = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['TEL'])\n",
        "data_urc = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['URC'])\n",
        "data_wlcon = getHeadlinesAsDictionary(start_date, end_date, keywords_dict['WLCON'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd3fbc5a",
      "metadata": {
        "id": "fd3fbc5a"
      },
      "source": [
        "## Sentiment Analysis Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3681e1",
      "metadata": {
        "id": "1b3681e1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279a84ef",
      "metadata": {
        "id": "279a84ef"
      },
      "source": [
        "Pre Trained model found in: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a399c34c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a399c34c",
        "outputId": "0d74c100-be51-447d-8ca4-3e88593c9f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n",
            "Date: 2023-11-24\n"
          ]
        }
      ],
      "source": [
        "dataset_list = {'ac': data_ac, 'acen': data_acen, 'aev': data_aev, 'agi': data_agi,\n",
        "            'ali': data_ali, 'bdo': data_bdo, 'bloom': data_bloom, 'bpi': data_bpi,\n",
        "            'cnpf': data_cnpf, 'cnvrg': data_cnvrg, 'dmc': data_dmc, 'emi': data_emi,\n",
        "            'glo': data_glo, 'gtcap': data_gtcap, 'ict': data_ict, 'jfc': data_jfc,\n",
        "            'jgs': data_jgs, 'ltg': data_ltg, 'mbt': data_mbt, 'mer': data_mer,\n",
        "            'monde': data_monde, 'nikl': data_nikl, 'pgold': data_pgold, 'scc': data_scc,\n",
        "            'sm': data_sm, 'smc': data_smc, 'smph': data_smph, 'tel': data_tel,\n",
        "            'urc': data_urc, 'wlcon': data_wlcon}  # Add all your datasets to this dictionary with corresponding names\n",
        "\n",
        "specific_model = pipeline(model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
        "\n",
        "for name, data in dataset_list.items():\n",
        "    sentiments = {}\n",
        "    for x, y in data.items():\n",
        "        print('Date: ' + x)\n",
        "        y['sentiments'] = specific_model(y['headlines'])\n",
        "\n",
        "    # Assuming convertToJson is a function you have defined\n",
        "    output_filename = f\"{name}.json\"\n",
        "    convertToJson(data, output_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a list of dataset filenames\n",
        "dataset_filenames = ['/content/ac.json', '/content/acen.json', '/content/aev.json', '/content/agi.json',\n",
        "                     '/content/ali.json', '/content/bdo.json', '/content/bloom.json', '/content/bpi.json',\n",
        "                     '/content/cnpf.json', '/content/cnvrg.json', '/content/dmc.json', '/content/emi.json',\n",
        "                     '/content/glo.json', '/content/gtcap.json', '/content/ict.json', '/content/jfc.json',\n",
        "                     '/content/jgs.json', '/content/ltg.json', '/content/mbt.json', '/content/mer.json',\n",
        "                     '/content/monde.json', '/content/nikl.json', '/content/pgold.json', '/content/scc.json',\n",
        "                     '/content/sm.json', '/content/smc.json', '/content/smph.json', '/content/tel.json',\n",
        "                     '/content/urc.json', '/content/wlcon.json']\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "for filename in dataset_filenames:\n",
        "    # Load JSON data from file\n",
        "    with open(filename) as f:\n",
        "        data = pd.read_json(f)\n",
        "\n",
        "    # Transpose the DataFrame\n",
        "    df_transposed = data.transpose()\n",
        "\n",
        "    # Convert the 'Date' index to datetime format\n",
        "    df_transposed.index = pd.to_datetime(df_transposed.index, errors='coerce')\n",
        "\n",
        "    # Extract 'label' and 'score' from 'Sentiment' column\n",
        "    df_transposed['Label'] = df_transposed['sentiments'].apply(lambda x: x[0]['label'] if isinstance(x, list) and len(x) > 0 else None)\n",
        "    df_transposed['Score'] = df_transposed['sentiments'].apply(lambda x: x[0]['score'] if isinstance(x, list) and len(x) > 0 else None)\n",
        "\n",
        "    # Drop the original 'Sentiment' column\n",
        "    df_transposed = df_transposed.drop(columns=['sentiments'])\n",
        "\n",
        "    df_transposed['Label_Sentiment'] = df_transposed['Label'].map({'neutral': 0, 'positive': 1, 'negative': -1})\n",
        "\n",
        "    # Extract the dataset name from the filename and make it uppercase\n",
        "    dataset_name = filename.split('/')[-1].split('.')[0].upper()\n",
        "\n",
        "    # Add a new column 'Name' and move 'Date' to the first column\n",
        "    df_transposed.insert(0, 'Date', df_transposed.index)\n",
        "    df_transposed.insert(1, 'Name', dataset_name)\n",
        "\n",
        "    # Save to CSV with a unique filename\n",
        "    output_filename = f\"{dataset_name}_output.csv\"\n",
        "    df_transposed.to_csv(output_filename, index=False)\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    dfs.append(df_transposed)\n",
        "\n",
        "# Concatenate all DataFrames in the list into one DataFrame\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save the merged DataFrame to a new CSV file\n",
        "merged_df.to_csv('merged_file_sentiments.csv', index=False)"
      ],
      "metadata": {
        "id": "QMlMSuM8CRte"
      },
      "id": "QMlMSuM8CRte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R9niaeaY8le",
        "outputId": "2eb60532-c309-4ff0-ea21-07af58027b04"
      },
      "id": "6R9niaeaY8le",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30 entries, 0 to 29\n",
            "Data columns (total 6 columns):\n",
            " #   Column           Non-Null Count  Dtype         \n",
            "---  ------           --------------  -----         \n",
            " 0   Date             30 non-null     datetime64[ns]\n",
            " 1   Name             30 non-null     object        \n",
            " 2   headlines        30 non-null     object        \n",
            " 3   Label            1 non-null      object        \n",
            " 4   Score            1 non-null      float64       \n",
            " 5   Label_Sentiment  1 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(2), object(3)\n",
            "memory usage: 1.5+ KB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}